{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "characteristic-approval",
   "metadata": {},
   "source": [
    "# Time Series - Stock Price Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "atmospheric-separation",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "from itertools import permutations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "from pmdarima.arima import auto_arima\n",
    "import statsmodels.graphics.tsaplots as tsaplot\n",
    "from statsmodels.tsa.holtwinters import Holt, ExponentialSmoothing, SimpleExpSmoothing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-impression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define plotting parameters and custom color palette \n",
    "cmaps_hex = ['#193251','#FF5A36','#1E4485', '#99D04A','#FF5A36', '#DB6668']\n",
    "#sns.set_palette(palette=cmaps_hex)\n",
    "sns_c = sns.color_palette(palette=cmaps_hex)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "wanted-bennett",
   "metadata": {},
   "source": [
    "## Import and Visualize the Data\n",
    "\n",
    "In this notebook we will work with a stock price dataset, which we obtained from [quandl](https://www.quandl.com/sign-up)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/stock_data.csv', index_col='Date')\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "passive-hamilton",
   "metadata": {},
   "source": [
    "We can decompose and plot our data to get an overview over the observed data, the trend, seasonality and residuals.\n",
    "\n",
    "**RECAP**: \n",
    "* *Trend*: Increase and decrease in the value of the data. It can further be divided into global and local trends. \n",
    "* *Seasonality*: Repetitive pattern of fixed frequency that is visible in the data.\n",
    "* *Noise/Resiudals*: Random data that can be obtained after extracting the trend and seasonal component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addressed-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check decomposition of trend, seasonality and residue of original time series\n",
    "decomposition = seasonal_decompose(x=df['Close'], \n",
    "                                   model='multiplicative',\n",
    "                                   period=30)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(4, 1, figsize=(12, 12), constrained_layout=True)\n",
    "decomposition.observed.plot(c=sns_c[0], ax=ax[0])\n",
    "ax[0].set(title='Close')\n",
    "decomposition.trend.plot(c=sns_c[1], ax=ax[1])\n",
    "ax[1].set(title='trend')\n",
    "decomposition.seasonal.plot(c=sns_c[2], ax=ax[2])\n",
    "ax[2].set(title='seasonal')\n",
    "decomposition.resid.plot(c=sns_c[3], ax=ax[3])\n",
    "ax[3].set(title='residual')\n",
    "fig.set_size_inches(20, 10);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "subsequent-meeting",
   "metadata": {},
   "source": [
    "The trend experienced some ups and downs as a stock generally does. We can conclude that it is not seasonal as the seasonality doesn't give any clear picture. The variance of the resiudals remain relatively constant except for some periods with outliers. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "unexpected-mainstream",
   "metadata": {},
   "source": [
    "## Check for Stationarity\n",
    "\n",
    "In order to apply an **ARIMA** model on the data we have to check if the time series is stationary. If not, we have to make it stationary. \n",
    "\n",
    "**RECAP**: \n",
    "* *Stationarity*: A time series is stationary if its statistical properties (e.g. mean, variance, etc.) are the same throughout the series, independently of the point in time where you observe it. There are no long-term predictable patterns such as trend or seasonality. Plots will show a roughly horizontal trend with constant variance. \n",
    "\n",
    "In order to check for stationarity we will apply three different methods. First, we will calculate and plot the rolling mean and rolling standard deviation. It they show an upward or downward trend or vary over time, then it is highly likely that our time series is non-stationary. Furthermore, we will use the **ADF** (Augmented Dickey-Fuller) and the **KPSS** (Kwiatkowski-Phillips-Schmidt-Shintests) tests. We have to be careful when interpreting the results: the null-hypothesis of the ADF test is that the time series is non-stationary whereas the KPSS test assumes that it is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for stationarity \n",
    "\n",
    "def stationarity_test(stock_close_price):\n",
    "    \n",
    "    # Calculate rolling mean and rolling standard deviation\n",
    "    rolling_mean = stock_close_price.rolling(30).mean()\n",
    "    rolling_std_dev = stock_close_price.rolling(30).std()\n",
    "    \n",
    "    # Plot the statistics\n",
    "    plt.figure(figsize=(24,6))\n",
    "    plt.plot(rolling_mean, color='#FF5A36', label='Rolling Mean')\n",
    "    plt.plot(rolling_std_dev, color='#1E4485', label = 'Rolling Std Dev')\n",
    "    plt.plot(stock_close_price, color='#99D04A',label='Original Time Series')\n",
    "    plt.xticks([])\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    \n",
    "    # ADF test\n",
    "    print(\"ADF Test:\")\n",
    "    adf_test = adfuller(stock_close_price,autolag='AIC')\n",
    "    print('Null Hypothesis: Not Stationary')\n",
    "    print('ADF Statistic: %f' % adf_test[0])\n",
    "    print('p-value: %f' % adf_test[1])\n",
    "    print('----'*10)\n",
    "    \n",
    "    # KPSS test\n",
    "    print(\"KPSS Test:\")\n",
    "    kpss_test = kpss(stock_close_price, regression='c', nlags=\"legacy\", store=False)\n",
    "    print('Null Hypothesis: Stationary')\n",
    "    print('KPSS Statistic: %f' % kpss_test[0])\n",
    "    print('p-value: %f' % kpss_test[1])\n",
    "    print('----'*10)\n",
    "    \n",
    "stationarity_test(df['Close'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "strong-bloom",
   "metadata": {},
   "source": [
    "The rolling mean moves up and down over time as the price of the stock varies. The p-value of the ADF test is > 0.05 which tells us that we cannot decline the null-hypothesis that the time series is non-stationary. On the other hand the p-value for the KPSS test is below 0.05 which means we can reject this null-hypothesis. All three tests lead to the result that this time series is not stationary.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "hairy-render",
   "metadata": {},
   "source": [
    "## De-trend the Time Series\n",
    "\n",
    "We need to de-trend the time series. Therefore we have several options. \n",
    "\n",
    "**Differencing**: A new series is constructed by calculating the value at the current time by differencing the value of actual observation of current time and its previous time. $$value(t) = actual\\_observation(t) - actual\\_observation(t-1)$$\n",
    "\n",
    "\n",
    "**Transformation**: Transforming the data using power, log, square root can help to linearize the data.\n",
    "\n",
    "**Seasonal Differencing**: The values of the time series are calculated by differencing between one observation and its previous Nth observation. $$value(t) = actual\\_observation(t) - actual\\_observation(t-N)$$\n",
    "\n",
    "\n",
    "**Fitting a model**: A linear regression model can be fitted to the time series. It will fit a linear trend on the time series. The values for the de-trended time series can be calculated by subtracting the actual observations with the values predicted by the model.\n",
    "$$value(t) = actual\\_observation(t) - predicted(t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# De-trending the time series\n",
    "df['Close_Detrend'] = (df['Close'] - df['Close'].shift(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for stationarity after de-trending \n",
    "def stationarity_test(stock_close_price):\n",
    "    \n",
    "    # Calculate rolling mean and rolling standard deviation\n",
    "    rolling_mean = stock_close_price.rolling(30).mean()\n",
    "    rolling_std_dev = stock_close_price.rolling(30).std()\n",
    "  \n",
    "    # Plot the statistics\n",
    "    plt.figure(figsize=(24,6))\n",
    "    plt.plot(rolling_mean, label='Rolling Mean',linewidth=2.0)\n",
    "    plt.plot(rolling_std_dev, label = 'Rolling Std Dev',linewidth=2.0)\n",
    "    plt.plot(stock_close_price,label='De-Trended Time Series')\n",
    "    plt.xticks([])\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # ADF test\n",
    "    print(\"ADF Test:\")\n",
    "    adf_test = adfuller(stock_close_price,autolag='AIC')\n",
    "    print('Null Hypothesis: Not Stationary')\n",
    "    print('ADF Statistic: %f' % adf_test[0])\n",
    "    print('p-value: %f' % adf_test[1])\n",
    "    print('----'*10)\n",
    "    \n",
    "    # KPSS test\n",
    "    print(\"KPSS Test:\")\n",
    "    kpss_test = kpss(stock_close_price, regression='c', nlags='legacy', store=False)\n",
    "    print('Null Hypothesis: Stationary')\n",
    "    print('KPSS Statistic: %f' % kpss_test[0])\n",
    "    print('p-value: %f' % kpss_test[1])\n",
    "    print('----'*10)\n",
    "    \n",
    "stationarity_test(df['Close_Detrend'].dropna())\n",
    "\n",
    "# Partial Autocorrelation Plot\n",
    "pacf = plot_pacf(df['Close_Detrend'].dropna(), lags=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "brief-cemetery",
   "metadata": {},
   "source": [
    "After de-trending the time series the AFD test as well as the KPSS test both indicate that our series is now stationary. Having a look at the partial autocorrelation plot suggests that correlation exists at certain lags."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "accompanied-preservation",
   "metadata": {},
   "source": [
    "## Split the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "sporting-compromise",
   "metadata": {},
   "source": [
    "We will split our data and take the first part as our training set and the years after 2016 as a test set. Since we are dealing with a time series here we cannot split randomly as we are used to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-rover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "df_arima = df['Close']\n",
    "train_test_split_ratio = int(len(df_arima)*0.8)\n",
    "train_data, test_data = df_arima[:train_test_split_ratio], df_arima[train_test_split_ratio:]\n",
    "\n",
    "# Plotting the train and test set\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Closing Price')\n",
    "plt.xticks([])\n",
    "plt.plot(train_data, 'red', label='Train data')\n",
    "plt.plot(test_data, 'black', label='Test data')\n",
    "plt.legend();"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eastern-constant",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "After splitting the data we can now try to forcast the prices. We can use smoothing methods and ARIMA methods. While smoothing methods can be used for non-stationray data, ARIMA requires the data to be stationary. `auto_arima` can help us to make the series stationary and determine the optimal order for the ARIMA model.\n",
    "\n",
    "We will perform multiple fits for each model to find the best hyperparamters. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "orange-marks",
   "metadata": {},
   "source": [
    "### Simple Exponential Smoothing\n",
    "\n",
    "Simple Exponential Smoothing (SES) is used when the data doesn't contain any trend or seasonality. Smoothing Factor for level ($\\alpha$) provides weightage to the influence of the observations. With larger values of $\\alpha$ more attention is given to the most recent past observations whereas smaller values indicate that more past observations are being considered for forecasting. \n",
    "\n",
    "$$F_{t+1} = \\alpha d_t + (1 - \\alpha)F_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-dependence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Exponential Smoothing Method\n",
    "pred_values = test_data.copy()\n",
    "pred_values = pd.DataFrame(pred_values)\n",
    "\n",
    "simple_exponential_df = pd.DataFrame(columns = ['RMS','Smoothing Level'])\n",
    "\n",
    "from itertools import permutations\n",
    "perm = permutations(list(np.linspace(0.05,1,num=20)), 1)\n",
    "for i in list(perm):\n",
    "    fit_sim_exp = SimpleExpSmoothing(np.asarray(train_data)).fit(smoothing_level = i[0])\n",
    "    pred_values['Simple_Exponential'] = fit_sim_exp.forecast(len(test_data))\n",
    "\n",
    "    rms = round(sqrt(mean_squared_error(test_data.values, pred_values.Simple_Exponential)),3)\n",
    "    simple_exponential_df = pd.concat([simple_exponential_df, pd.DataFrame({'RMS' : [rms] , 'Smoothing Level' : [i[0]]})], ignore_index=True)\n",
    "\n",
    "opt_values = simple_exponential_df.loc[simple_exponential_df['RMS'] == min(simple_exponential_df['RMS']),['Smoothing Level']].values\n",
    "\n",
    "\n",
    "# Use optimised values from the lists\n",
    "fit_sim_exp = SimpleExpSmoothing(np.asarray(train_data)).fit(smoothing_level = opt_values[0][0])\n",
    "pred_values['Simple_Exponential'] = fit_sim_exp.forecast(len(test_data))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train_data, label='Train')\n",
    "plt.plot(test_data, label='Test')\n",
    "plt.plot(pred_values['Simple_Exponential'], label='Simple_Exponential')\n",
    "plt.xticks([])\n",
    "plt.legend(loc='best')\n",
    "\n",
    "rms_sim_exp = sqrt(mean_squared_error(test_data.values, pred_values.Simple_Exponential))\n",
    "print(\"Simple Exponential Smoothing RMS :- \" + str(round(rms_sim_exp,3)) + \" & Smoothing Level :- \"+str(round(opt_values[0][0],3)))\n",
    "\n",
    "plt.savefig(\"visualisations/Stock_Simple_Exponential.png\",dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abandoned-purple",
   "metadata": {},
   "source": [
    "### Holt's Exponential Smoothing\n",
    "\n",
    "Holt’s Exponential Smoothing takes the trend into account for forecasting the time series. We can use it when we have a trend but no seasonality in the data. It calculates the Smoothing value, which is the same used in SES for forecasting (first equation). Trend Coefficient (β) provides weightage to the difference in the consequent smoothing values and the previous trend estimate. The forecasting is a combination of the smoothing value and the trend estimate.\n",
    "\n",
    "$$S_t = \\alpha d_t + (1 - \\alpha)F_t$$\n",
    "$$b_t = \\beta (S_t - S_{t-1}) + (1 - \\beta) b_{t-1}$$\n",
    "$$F_{t+1} = S_t + b_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holt's Exponential Smoothing Method\n",
    "holt_linear_df = pd.DataFrame(columns = ['RMS','Smoothing Level','Smoothing Slope'])\n",
    "\n",
    "perm = permutations(list(np.linspace(0.05,1,num=20)), 2)\n",
    "for i in list(perm):\n",
    "    fit_holt = Holt(np.asarray(train_data)).fit(smoothing_level = i[0],smoothing_slope=i[1])\n",
    "    pred_values['Holt_linear'] = fit_holt.forecast(len(test_data))\n",
    "\n",
    "    rms = round(sqrt(mean_squared_error(test_data.values, pred_values.Holt_linear)),3)\n",
    "    #holt_linear_df = holt_linear_df.append(other = {'RMS' : rms , 'Smoothing Level' : i[0], 'Smoothing Slope':i[1]} , ignore_index=True)\n",
    "    holt_linear_df = pd.concat([holt_linear_df, pd.DataFrame({'RMS' : [rms] , 'Smoothing Level' : [i[0]], 'Smoothing Slope':[i[1]]})], ignore_index=True)\n",
    "\n",
    "opt_values = holt_linear_df.loc[holt_linear_df['RMS'] == min(holt_linear_df['RMS']),['Smoothing Level','Smoothing Slope']].values\n",
    "\n",
    "\n",
    "# Using optimised values from the lists.\n",
    "fit_holt = Holt(np.asarray(train_data)).fit(smoothing_level = opt_values[0][0],smoothing_slope=opt_values[0][1])\n",
    "pred_values['Holt_linear'] = fit_holt.forecast(len(test_data))\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(train_data, label='Train')\n",
    "plt.plot(test_data, label='Test')\n",
    "plt.plot(pred_values['Holt_linear'], label='Holt_linear')\n",
    "plt.xticks([])\n",
    "plt.legend(loc='best')\n",
    "plt.title('Holt Exponential Smoothing')\n",
    "plt.savefig(\"visualisations/Stock_Holt_winters.png\",dpi=300)\n",
    "\n",
    "rms_holt_exp = sqrt(mean_squared_error(test_data.values, pred_values.Holt_linear))\n",
    "print(\"Holt’s Exponential Smoothing RMS :- \" + str(round(rms_holt_exp,3)) + \" & Smoothing Level :- \"+str(round(opt_values[0][0],3)) + \" & Smoothing Slope :- \"+str(round(opt_values[0][1],3)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "known-musician",
   "metadata": {},
   "source": [
    "### Auto-Regressive Integrated Moving Average (ARIMA)\n",
    "\n",
    "ARIMA model is a combination of Auto-Regressive and Moving Average model along with the Integration of differencing. Auto-Regressive model determines the relationship between an observation and a certain number of lagged observations. The Integrated part is the differencing of the actual observations to make the time series stationary. Moving Average determines the relationship between an observation and residual error obtained by using a moving average model on the lagged observations.\n",
    "\n",
    "* *Auto-Regressive (p)*: Number of lag observations in the model. Also called lag order.\n",
    "* *Integrated (d)*: Number of times the actual observations are differenced for stationarity. Also called degree of differencing.\n",
    "* *Moving Average (q)*: Size of moving average window. Also called the order of moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto ARIMA Method\n",
    "arima_model = auto_arima(train_data,\n",
    "                      start_p=1, start_q=1,\n",
    "                      max_p=5, max_q=5,\n",
    "                      test='adf',        \n",
    "                      trace=True,\n",
    "                      alpha=0.05,\n",
    "                      scoring='mse',\n",
    "                      suppress_warnings=True,\n",
    "                      seasonal = False\n",
    "                      )\n",
    "\n",
    "# Fit the final model with the order\n",
    "fitted_model = arima_model.fit(train_data) \n",
    "print(fitted_model.summary())\n",
    "\n",
    "# Forecasting values\n",
    "forecast_values = fitted_model.predict(len(test_data), alpha=0.05) \n",
    "# fcv_series = pd.Series(forecast_values[0], index=test_data.index)\n",
    "fcv_series = forecast_values\n",
    "\n",
    "# Plot the predicted stock price and original price\n",
    "plt.figure(figsize=(12,5), dpi=100)\n",
    "plt.plot(train_data, label='training')\n",
    "plt.plot(test_data, label='Actual Stock Price')\n",
    "plt.plot(fcv_series,label='Predicted Stock Price')\n",
    "plt.title('Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Close Price')\n",
    "plt.xticks([])\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "#plt.show()\n",
    "\n",
    "# Evaluate the model by calculating RMSE\n",
    "rms_auto_arima = sqrt(mean_squared_error(test_data.values, fcv_series))\n",
    "print(\"Auto-Arima RMSE :- \" + str(round(rms_auto_arima,3)))\n",
    "\n",
    "plt.savefig(\"visualisations/Stock_ARIMA.png\",dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "british-ultimate",
   "metadata": {},
   "source": [
    "## Evaluation of the Models\n",
    "\n",
    "To evaluate the performance of the model, we will use Root Mean Squared Error (RMSE) and compare which model performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing RMSE of all the methods\n",
    "print(\"RMSE of all the methods\")\n",
    "print(\"Auto-Arima: \", round(rms_auto_arima,3))\n",
    "print(\"Simple Exponential Smoothing: \", round(rms_sim_exp,3))\n",
    "print(\"Holt’s Exponential Smoothing: \", round(rms_holt_exp,3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "stopped-setting",
   "metadata": {},
   "source": [
    "From the three models we trained the Holt's Exponential Smoothing reached the smalles RSME. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9093bdaa5759c483585231923ef43203d87ed9a183b93a1933f7ead90b6b5235"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
